{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47c0b2-1e0d-4015-a14a-63102a33ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "import numpyro\n",
    "numpyro.set_host_device_count(8)\n",
    "import os\n",
    "#os.environ['PYTENSOR_FLAGS'] = 'floatX=float32'\n",
    "import warnings\n",
    "\n",
    "import sys\n",
    "from pyprojroot import here\n",
    "sys.path.append(\"..\") \n",
    "from laos_gggi.statistics import get_distance_to_rivers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import pytensor\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "from laos_gggi import load_all_data, load_gpcc_data, load_emdat_data, load_shapefile, load_wb_data\n",
    "from laos_gggi.data_functions.rivers_data_loader import load_rivers_data\n",
    "from laos_gggi.data_functions.rivers_damage import create_hydro_rivers_damage, create_floods_rivers_damage\n",
    "from laos_gggi.replication_data import create_replication_data\n",
    "from laos_gggi.const_vars import COUNTRIES_ISO, ISO_DICTIONARY, LAOS_LOCATION_DICTIONARY\n",
    "from laos_gggi.plotting import configure_plot_style, plot_ppc_loopit\n",
    "from laos_gggi.sample import sample_or_load\n",
    "from laos_gggi.statistics import get_distance_to_rivers\n",
    "\n",
    "\n",
    "configure_plot_style()\n",
    "SEED = sum(list(map(ord, 'climate_bayes')))\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dbe1b-2dcd-46bd-a3de-2dd593c37ac7",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae2cc6-1e91-48e4-82ef-dbf177539f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_country_effect():\n",
    "  with pm.modelcontext(None):\n",
    "    country_effect_mu = pm.Normal('country_effect_mu', mu = 0, sigma = 1)\n",
    "    country_effect_scale = pm.Gamma('country_effect_scale', alpha=2, beta=1)\n",
    "    country_effect_offset = pm.Normal('country_effect_offset', sigma=1, dims=\"ISO\")\n",
    "    country_effect = pm.Deterministic('country_effect', country_effect_mu + country_effect_scale * country_effect_offset, dims=\"ISO\")\n",
    "  return country_effect, country_effect_mu, country_effect_scale, country_effect_offset\n",
    "\n",
    "#Redefine add data\n",
    "def add_data(features: list[str], df: pd.DataFrame, \n",
    "             target: str | None = None, name = None, dims=None, dtype=None):\n",
    "    X_name = 'X' if name is None else f'X_{name}'\n",
    "    Y_name = 'Y' if name is None else f'Y_{name}'\n",
    "    \n",
    "    if dtype is None:\n",
    "        dtype = pytensor.config.floatX\n",
    "        \n",
    "    with pm.modelcontext(None):\n",
    "        X = pm.Data(X_name, df[features].astype(dtype), dims=dims)\n",
    "        \n",
    "        if target is not None:\n",
    "            Y = pm.Data(Y_name, df[target].astype(dtype), dims=dims[0] if dims is not None else dims)\n",
    "            return X, Y\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d49ad-5af1-4df6-b2dc-4508671024fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot inputs\n",
    "def generate_plot_inputs(target_variable: str, idata, disaster_type: str = \"hydrological_disasters\", df = pd.DataFrame ):\n",
    "    #Extract predictions\n",
    "    predictions = idata.posterior_predictive['damage'].mean(dim=['chain', 'draw'])\n",
    "    predictions = predictions.to_dataframe().drop(columns = [\"year\", \"ISO\"]).reset_index().rename(\n",
    "                        columns = {\"damage\": \"predictions\"})\n",
    "\n",
    "    hdi_mean = az.hdi(idata.posterior_predictive.damage)\n",
    "\n",
    "    hdi = hdi_mean['damage'].to_dataframe().drop(columns = [\"year\", \"ISO\"]).reset_index()\n",
    "\n",
    "    hdi_mean_50 = az.hdi(idata.posterior_predictive.damage, hdi_prob=.5)\n",
    "    \n",
    "    hdi_50 = hdi_mean_50['damage'].to_dataframe().drop(columns = [\"year\", \"ISO\"]).reset_index()\n",
    "\n",
    "    #Merge results and predictions in one df\n",
    "    df_predictions = df[[target_variable, \"ISO\", \"year\"]]\n",
    "\n",
    "    #95% HDI\n",
    "    df_predictions = ( pd.merge(df_predictions,  hdi.query('hdi == \"lower\"')[[\"ISO\", \"year\", \"damage\"]] , \n",
    "             left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"damage\": \"lower_damage_95\"}))\n",
    "    df_predictions = (pd.merge(df_predictions, hdi.query('hdi == \"higher\"')[[\"ISO\", \"year\", \"damage\"]] ,\n",
    "         left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"damage\": \"higher_damage_95\"}))\n",
    "    #50% HDI\n",
    "    df_predictions = ( pd.merge(df_predictions,  hdi_50.query('hdi == \"lower\"')[[\"ISO\", \"year\", \"damage\"]] , \n",
    "             left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"damage\": \"lower_damage_50\"}))\n",
    "    df_predictions = (pd.merge(df_predictions, hdi_50.query('hdi == \"higher\"')[[\"ISO\", \"year\", \"damage\"]] ,\n",
    "         left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"damage\": \"higher_damage_50\"}))\n",
    "    \n",
    "    #Predictions\n",
    "    df_predictions = (pd.merge(df_predictions, predictions ,\n",
    "             left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "             .rename(columns = {\"damage\": \"predictions\"}))\n",
    "    \n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6588022-e40b-4186-9de9-7639b3b48de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function\n",
    "def plotting_function(idata, country: str, df: pd.DataFrame, target_variable: str):\n",
    "    df_predictions = generate_plot_inputs(idata = idata, df = df, target_variable = target_variable)\n",
    "\n",
    "    #Filter country\n",
    "    data = df_predictions.query(\"ISO == @country\")\n",
    "    # data[\"damage_adjusted\"] = (data[\"damage\"])\n",
    "    # cols_to_millions = [\"predictions\", \"damage_adjusted\", \"higher_damage_95\", \"lower_damage_95\", \"lower_damage_50\", \"higher_damage_50\"]\n",
    "    # data[cols_to_millions] = data[cols_to_millions] /1e6\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(data[\"year\"], data[\"predictions\"] , zorder=1000, color='tab:red', label=f'Mean  hydrometereological events damage')\n",
    "    ax.scatter(data[\"year\"], np.exp(data[target_variable].astype(float)) , color='k', label= (\"Real hydrometereological events damage in millions of dollars\"))\n",
    "    ax.fill_between(data[\"year\"], data[\"higher_damage_95\"] , data[\"lower_damage_95\"], alpha=0.25, color='tab:blue', label='95% HDI')\n",
    "    ax.fill_between(data[\"year\"], data[\"lower_damage_50\"], data[\"higher_damage_50\"], alpha=0.5, color='tab:blue', label='50% HDI')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    #plt.title(f\"{country} disaster count and predictions\")\n",
    "\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"hydrometereological events damage in millions of dollars\")\n",
    "    \n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95828d-39f4-46aa-ab90-a6e9a163f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_from_shape(shapefile, rivers, coastline, grid_size = 100):\n",
    "    long_min, lat_min, long_max, lat_max = shapefile.dissolve().bounds.values.ravel()\n",
    "    long_grid = np.linspace(long_min, long_max, grid_size)\n",
    "    lat_grid = np.linspace(lat_min, lat_max, grid_size)\n",
    "\n",
    "    grid = np.column_stack([x.ravel() for x in np.meshgrid(long_grid, lat_grid)])\n",
    "    grid = gpd.GeoSeries(gpd.points_from_xy(*grid.T), crs='EPSG:4326')\n",
    "    grid = gpd.GeoDataFrame({'geometry': grid})\n",
    "\n",
    "    point_overlay = grid.overlay(shapefile, how='intersection')\n",
    "    points = point_overlay.geometry\n",
    "    points = points.to_frame().assign(long = lambda x: x.geometry.x, lat = lambda x: x.geometry.y)\n",
    "    \n",
    "    #Obtain distance with rivers\n",
    "    distances_to_rivers = (get_distance_to(rivers, points=points, return_columns=['ORD_FLOW', 'HYRIV_ID'])\n",
    "                           .rename(columns={'distance_to_closest':'distance_to_river'}))\n",
    "\n",
    "    points = pd.merge(points, distances_to_rivers, left_index= True, right_index= True, how = \"left\")\n",
    "    \n",
    "    #Obtain Laos distance with coastlines\n",
    "    distances_to_coastlines = (get_distance_to(coastline.boundary, points=points, return_columns=None)\n",
    "                              .rename(columns={'distance_to_closest':'distance_to_coastline'}))\n",
    "\n",
    "    points =  pd.merge(points, distances_to_coastlines, left_index= True, right_index= True, how = \"left\")\n",
    "    \n",
    "    #Create log of distances\n",
    "    points = points.assign(log_distance_to_river = lambda x: np.log(x.distance_to_river),\n",
    "                           log_distance_to_coastline = lambda x: np.log(x.distance_to_coastline))\n",
    "    \n",
    "    if 'ISO_A3' in point_overlay.columns:\n",
    "        points['ISO'] = point_overlay.ISO_A3\n",
    "    else:\n",
    "        points['ISO'] = 'LAO'\n",
    "        \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56810344-87ad-49cf-94c2-b375244f759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_distance_to(gdf, points, return_columns=None, crs=\"EPSG:3395\", n_cores=-1):\n",
    "    if return_columns is None:\n",
    "        return_columns = []\n",
    "        \n",
    "    gdf_km = gdf.copy().to_crs(crs)\n",
    "    points_km = points.copy().to_crs(crs)\n",
    "    \n",
    "    def get_closest(idx, row, gdf_km, return_columns):\n",
    "        series = gdf_km.distance(row.geometry)\n",
    "        index = series[series == series.min()].index[0]\n",
    "\n",
    "        ret_vals = (series.min(), )\n",
    "        for col in return_columns:\n",
    "            ret_vals += (gdf_km.loc[index][col], )\n",
    "        \n",
    "        return ret_vals\n",
    "    \n",
    "    with Parallel(n_cores, require='sharedmem') as pool:\n",
    "        results = pool(delayed(get_closest)(idx, row, gdf_km, return_columns) for idx, row in tqdm(points_km.iterrows(), total=points.shape[0]))\n",
    "    return pd.DataFrame(results, columns = ['distance_to_closest'] + return_columns, index=points.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee4bf3-766c-420f-abb2-f69ca77ba254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_center(X):\n",
    "    return (pt.max(X, axis=0) + pt.min(X, axis=0)).eval() / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e570a-2101-47f7-8808-ed1ba888007a",
   "metadata": {},
   "source": [
    "# Importing and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0442a900-3717-4c98-a3ed-2ddf1b023fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_df = create_replication_data()\n",
    "emdat = load_emdat_data()\n",
    "wb = load_wb_data()\n",
    "\n",
    "#Load shapefiles\n",
    "world = load_shapefile('world', repair_ISO_codes=True)\n",
    "laos = load_shapefile('laos')\n",
    "coastline = load_shapefile('coastline')\n",
    "rivers = load_rivers_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878a7d2-9774-4ad4-879c-a43de6b5d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = load_shapefile('world')\n",
    "laos = load_shapefile('laos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e961e-6fbc-4bb4-833a-ea94cff7ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_df = create_hydro_rivers_damage()\n",
    "\n",
    "emdat_iso = damage_df.ISO.unique()\n",
    "world_iso = world.ISO_A3.unique()\n",
    "wb_iso = wb.index.get_level_values(0).unique()\n",
    "\n",
    "# Codes in shapefile but not in EMDAT\n",
    "common_codes = set(world_iso).intersection(set(emdat_iso)).intersection(set(wb_iso))\n",
    "damage_df = damage_df.query('ISO in @common_codes').copy()\n",
    "world = world.query('ISO_A3 in @common_codes').copy()\n",
    "wb = (wb.reset_index().query('country_code in @common_codes').assign(year = lambda x: pd.to_datetime(x.year, format='%Y')).copy())\n",
    "\n",
    "damage_df = (pd.merge(damage_df, wb, left_on=['ISO', 'year'], right_on=['country_code', 'year'], how='left')\n",
    "             .dropna(subset=['population_density', 'gdp_per_cap', 'Population']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50188a63-701d-423a-ac40-2cfa1d19d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load river files\n",
    "big_rivers = gpd.read_file(here('data/rivers/big_rivers.shp'))\n",
    "\n",
    "#Load Laos points\n",
    "laos_points = pd.read_csv(\"../data/laos_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b09a0b-26cb-422c-8df8-01cd87047990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Floods damages and people_affected\n",
    "damage_df_f = create_floods_rivers_damage()\n",
    "damage_df_f = pd.merge(damage_df_f, rep_df, right_on=[\"ISO\", \"year\"], left_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "\n",
    "#Create different \"ORDFLOW\" dummies\n",
    "damage_df_f[\"ORD_FLOW_1\"] = 0\n",
    "for i in damage_df_f.index.values:\n",
    "    if damage_df_f.loc[i, \"ORD_FLOW\"] == 1:\n",
    "        damage_df_f.loc[i, \"ORD_FLOW_1\"] = 1\n",
    "\n",
    "#Create the variable intrecation between closest river and ORD_FLOW\n",
    "damage_df_f[\"closest_river_int_ORD_FLOW_1\"] = damage_df_f[\"closest_river\"] * damage_df_f[\"ORD_FLOW_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74248745-125b-40d4-8808-18dece5e09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import disaster probability geospatial data\n",
    "merged_df5 = pd.read_csv(\"../data/HSGP_full_model_data_hydro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7e64f9-f682-487f-97e3-737765c4cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I define the data set used to train the standardization pipeline\n",
    "#Delimiting data set\n",
    "model_list_2 = ['ISO', 'Start_Year', \"is_disaster\", 'log_distance_to_river', 'log_distance_to_coastline',\n",
    "                'is_island', \"lat\", \"long\" ]\n",
    "\n",
    "\n",
    "distance_features_2 = ['log_distance_to_river', 'log_distance_to_coastline', 'is_island']\n",
    "\n",
    "time_varying_features_2 = [\"Population\", \"co2\", \"precip_deviation\", \"dev_ocean_temp\", 'log_population_density', 'log_population_density_squared',\n",
    "             'log_gdp_per_cap', 'log_gdp_per_cap_squared']\n",
    "\n",
    "model_df_2 = merged_df5[model_list_2 + time_varying_features_2].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4bb55-ac9e-4017-a124-277d36c95b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import economic and climate predictions for LAO\n",
    "LAO_economic_predictions = pd.read_csv(\"../data/economic_forecasts.csv\")\n",
    "LAO_climate_predictions = pd.read_csv(\"../data/climate_forecasts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72be46-cd7a-4beb-8e0e-8919adcb01c4",
   "metadata": {},
   "source": [
    "# Base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c9c14-6afd-497d-969a-6d5dc1fabb86",
   "metadata": {},
   "source": [
    "## Base damage model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53eba5-fef5-41eb-9670-0abfc112ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform damage to millions\n",
    "damage_df_f[\"log_Total_Damage_Adjusted\"] = np.log(damage_df_f[\"Total_Damage_Adjusted_hydro\"].replace({0.0: np.nan}))\n",
    "damage_df_f[\"log_Total_Damage_Adjusted_millions\"]  =  damage_df_f[\"log_Total_Damage_Adjusted\"] -np.log(1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165fa91-e96b-4ed2-9aa8-c0a82ea1d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Homogenize variable names\n",
    "variable_name_dict = {\"ln_population_density\":\"log_population_density\", \n",
    " \"ln_population_density_squared\":\"log_population_density_squared\",\n",
    " \"population\":\"Population\",\n",
    " \"ln_gdp_pc\":\"log_gdp_per_cap\",\n",
    " \"square_ln_gdp_pc\":\"log_gdp_per_cap_squared\",\n",
    " \"precip_deviation\":\"precip_deviation\",\n",
    "}\n",
    "\n",
    "damage_df_f = damage_df_f.rename(columns = variable_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd194d4f-236d-40cc-af8f-1151ac644685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the columns to use\n",
    "df_2 = damage_df_f[[\"log_population_density\",  \"Population\",  \"ISO\",\n",
    "                  \"year\" , \"log_gdp_per_cap\",\n",
    "                    \"log_Total_Damage_Adjusted\", \"log_Total_Damage_Adjusted_millions\",\n",
    "                 \"precip_deviation\", \"co2\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e2d32-10b3-49d9-b475-aa3e2c617d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pipeline, standardize df and obtain the squared variables\n",
    "cols_to_standardize = [\"log_population_density\",   \"Population\",\n",
    "                       \"log_gdp_per_cap\", \"precip_deviation\", \"co2\"]\n",
    "\n",
    "cols_to_square = [\"log_population_density\", \"log_gdp_per_cap\"]\n",
    "\n",
    "pipeline_damages = ColumnTransformer([('standardize', StandardScaler(), cols_to_standardize),],\n",
    "                            remainder='passthrough')\n",
    "#train pipeline and standardize: \n",
    "df_2_stand = pd.DataFrame(pipeline_damages.fit_transform(df_2 ),\n",
    "                          index=df_2.index,\n",
    "                          columns=[x.split('__')[-1] for x in pipeline_damages.get_feature_names_out()])\n",
    "\n",
    "#Square\n",
    "df_2_stand[\"log_population_density_squared\"] =df_2_stand[\"log_population_density\"] **2\n",
    "\n",
    "df_2_stand[\"log_gdp_per_cap_squared\"] = df_2_stand[\"log_gdp_per_cap\"] ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37f1ab-791e-4c8f-a7f4-7b3d2b38a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "damages_features = [\"log_population_density\", \"Population\",\n",
    "                       \"log_gdp_per_cap\", \"precip_deviation\", \"co2\",\n",
    "                   \"log_population_density_squared\", \"log_gdp_per_cap_squared\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8268d-5429-41fa-abd0-a5df5b0540bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set coords\n",
    "ISO_2_idx, ISO_2 =  pd.factorize(df_2_stand[\"ISO\"])\n",
    "coords_damage = {\n",
    "    'ISO':ISO_2,\n",
    "    'obs_idx':df_2_stand.index,\n",
    "    'feature': damages_features\n",
    "        }    \n",
    "\n",
    "xr_idx_damage = xr.Coordinates.from_pandas_multiindex(df_2_stand.set_index(['ISO', 'year']).index, 'obs_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07907c-e019-4905-9b95-9ba6d3893232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Damages model\n",
    "with pm.Model(coords=coords_damage) as simple_damages_model:\n",
    "    #Declare data\n",
    "    X, Y= add_data(features= coords_damage[\"feature\"] ,  target = \"log_Total_Damage_Adjusted_millions\", df = df_2_stand,\n",
    "                  dims = (\"obs_idx\", \"feature\" ))\n",
    "    \n",
    "    #Country effectz\n",
    "    country_effect = pm.Normal(\"country_effect\", mu = 0, sigma =3, dims = [\"ISO\"])\n",
    "\n",
    "    #Betas\n",
    "    betas = pm.Normal(\"beta\", mu = 0, sigma = 3, dims = [\"feature\"])\n",
    "\n",
    "    #Model mu\n",
    "    mu = country_effect[ISO_2_idx] + X @ betas\n",
    "\n",
    "    #Sigma\n",
    "    sigma = pm.Exponential('sigma', lam=1/10)\n",
    "\n",
    "    log_damage = pm.Normal(\"log_damage\", observed= Y, mu = mu, sigma = sigma , dims = [\"obs_idx\"])\n",
    "    \n",
    "    damage = pm.Deterministic('damage', pt.exp(log_damage), dims = [\"obs_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f39c4-9a5a-48dd-8f04-71c8a04aa741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling\n",
    "with simple_damages_model:\n",
    "    simple_damages_idata =sample_or_load(\n",
    "            fp=\"simple_damages_model7_short\",\n",
    "            resample= True,\n",
    "            sample_kwargs={\n",
    "                \"nuts_sampler\": 'numpyro',\n",
    "                \"chains\": 4,\n",
    "               #\"target_accept\": 0.9,\n",
    "                \"draws\": 500,\n",
    "            })\n",
    "\n",
    "    simple_damages_idata = simple_damages_idata.assign_coords(xr_idx_damage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2129d5-b260-4697-8704-ac9648742dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(simple_damages_idata, [\"~country_effect\", \"~damage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc62567d-7cec-4039-a810-af3033dd322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(simple_damages_idata, var_names = [\"~country_effect\",\n",
    "                                                  \"~sigma\", \"~damage\"], combined = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd35fbf-7a9c-40ff-be2e-e0b318d65aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(simple_damages_idata, var_names =  [\"~country_effect\", \"~damage\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab15b65-2b6f-4f2e-bd24-e8f2cbd3de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(simple_damages_idata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7a83f-a57f-4d2b-a81e-ca2eba0e7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plotting_function(idata =simple_damages_idata , \n",
    "                      country = \"LAO\",\n",
    "                      df = df_2_stand ,\n",
    "                     target_variable= \"log_Total_Damage_Adjusted_millions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06765848-dfc4-4327-8c02-e4ea41e7c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    plotting_function(idata =simple_damages_idata , \n",
    "                      country = \"VNM\",\n",
    "                      df = df_2_stand ,\n",
    "                     target_variable= \"log_Total_Damage_Adjusted_millions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87bce0-68f1-4840-8cca-9bfa3b5150d1",
   "metadata": {},
   "source": [
    "# Damage curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a11ae4-0033-4e80-9788-6870251dd3a9",
   "metadata": {},
   "source": [
    "### Join idatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7836a5-dd64-4a15-8858-655c6601a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read idata \n",
    "floatX = pytensor.config.floatX\n",
    "HSGP_full_model_idata = az.from_netcdf(\"../data/HSGP_full_model_idata.netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ebca9f-52bf-45f7-b327-de95fd89f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename ISO dimension to avoid conflicts merging\n",
    "simple_damages_idata =simple_damages_idata.rename({\"ISO\": \"ISO_damage\",\n",
    "                                                   \"country_effect\": \"country_effect_damage\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193464f7-9a0e-4afe-ad06-f79b1f34c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge posteriors\n",
    "merged_posteriors = xr.merge([HSGP_full_model_idata.sel(ISO = coords_predictions[\"ISO\"]).posterior,\n",
    "                              simple_damages_idata.posterior])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350f93d-7c12-4eb6-9937-33cbe619d971",
   "metadata": {},
   "source": [
    "### Set data for events model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84103c-5335-4de0-b2ec-340d142fb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the used fd\n",
    "#Delimiting data set\n",
    "model_list_2 = ['ISO', 'Start_Year', \"is_disaster\", 'log_distance_to_river', 'log_distance_to_coastline',\n",
    "                'is_island', \"lat\", \"long\" ]\n",
    "\n",
    "\n",
    "distance_features_2 = ['log_distance_to_river', 'log_distance_to_coastline', 'is_island']\n",
    "\n",
    "time_varying_features_2 = [\"Population\", \"co2\", \"precip_deviation\", \"dev_ocean_temp\", 'log_population_density','log_gdp_per_cap']\n",
    "\n",
    "events_df = merged_df5[model_list_2 + time_varying_features_2].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a3d7b-2119-4a32-ac68-554da0f3fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define pipeline, standardize df and obtain the squared variables\n",
    "cols_to_standardize = distance_features_2 + time_varying_features_2\n",
    "\n",
    "cols_to_square = [\"log_population_density\", \"log_gdp_per_cap\"]\n",
    "\n",
    "pipeline_events = ColumnTransformer([('standardize', StandardScaler(), cols_to_standardize),\n",
    "                                     ('square', PolynomialFeatures(degree= (2,2), include_bias= False, ),cols_to_square)],\n",
    "                            remainder='passthrough')\n",
    "#train pipeline and standardize: \n",
    "events_df_stand = pd.DataFrame(pipeline_events.fit_transform(events_df),\n",
    "                          index=events_df.index,\n",
    "                          columns=[x.split('__')[-1] for x in pipeline_events.get_feature_names_out()])\n",
    "\n",
    "#Rename\n",
    "events_df_stand.rename( columns = {\"log_population_density^2\": \"log_population_density_squared\",\n",
    "                             \"log_gdp_per_cap^2\": \"log_gdp_per_cap_squared\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba48fb3-b373-4fd1-9fb1-126c8fbcba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define general disaster cooords\n",
    "isa_disaster_idx , is_disaster = pd.factorize(events_df_stand[\"is_disaster\"])\n",
    "ISO_idx, ISO = pd.factorize(events_df_stand[\"ISO\"]) \n",
    "LAOS_idx = ISO.tolist().index(\"LAO\")\n",
    "obs_idx = events_df_stand.index\n",
    "gp_features = [\"lat\", \"long\"]\n",
    "\n",
    "\n",
    "event_features = [\"log_population_density\", \"log_population_density_squared\", \"Population\" , \n",
    "                       \"log_gdp_per_cap\", \"log_gdp_per_cap_squared\", \"precip_deviation\", \"co2\" ]\n",
    "\n",
    "#Creating idx\n",
    "xr_idx_events = xr.Coordinates.from_pandas_multiindex(events_df_stand.set_index(['ISO', 'Start_Year']).index, 'obs_idx')\n",
    "\n",
    "#Set coords\n",
    "coords_predictions = {\"is_disaster\" : is_disaster,\n",
    "        \"obs_idx\": obs_idx,\n",
    "        \"ISO\": ISO,\n",
    "        \"distance_features\": distance_features_2,\n",
    "        \"time_varying_features\" : time_varying_features_2,\n",
    "        \"gp_feature\":gp_features,\n",
    "        \"ISO_damage\": coords_damage[\"ISO\"],\n",
    "        \"new_damage_features\": event_features,\n",
    "        \"year\":events_df_stand[\"Start_Year\"].unique()\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d572e-42c6-4d46-adcb-aebf3bc64936",
   "metadata": {},
   "source": [
    "### Select points for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64a219-fce1-4679-83b6-0efdfc26eeef",
   "metadata": {},
   "source": [
    "Define the points, country and year for the curve construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0cff5-754f-482b-9aab-4b2adf9390b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare point for predictions\n",
    "points_df = pd.DataFrame({'Vientiane': {'lat': 17.9757, 'long': 102.6331},\n",
    "                            'Luang Prabang': {'lat': 19.8860, 'long': 102.1350},\n",
    "                            'Savannakhet': {'lat': 16.5575, 'long': 104.7528},\n",
    "                            'Pakse': {'lat': 15.1202, 'long': 105.7987},\n",
    "                            'Thakhek': {'lat': 17.4106, 'long': 104.8304},\n",
    "                        }).transpose().reset_index().rename(columns = {\"index\": \"city\"})\n",
    "\n",
    "\n",
    "points_df_geo =  gpd.GeoDataFrame(\n",
    "                points_df, geometry=gpd.points_from_xy(points_df[\"long\"], points_df[\"lat\"]), crs=\"EPSG:4326\"\n",
    "            )\n",
    "\n",
    "\n",
    "#Get distances\n",
    "log_point_distance_rivers = np.log(get_distance_to(rivers.boundary, points=points_df_geo, return_columns=None)\n",
    "                              .rename(columns={'distance_to_closest':'log_distance_to_river'}))\n",
    "\n",
    "log_point_distance_coastlines  = np.log(get_distance_to(coastline.boundary, points=points_df_geo, return_columns=None)\n",
    "                              .rename(columns={'distance_to_closest':'log_distance_to_coastline'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e243d2a-1efe-4305-a5d5-1562e9a4859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge distances\n",
    "point_distance_data =  pd.merge(log_point_distance_rivers, log_point_distance_coastlines,\n",
    "                                left_index=True, right_index= True, how= \"inner\")\n",
    "\n",
    "point_distance_data[\"is_island\"] = False\n",
    "point_distance_data.rename(columns = { \"log_distance_to_river\": \"log_distance_to_river\",\n",
    "                                     }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4007a9f-d6f6-4959-9fa1-15e3718409be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize distances\n",
    "distances_to_standardize  = [\"log_distance_to_river\", \"log_distance_to_coastline\" ]\n",
    "\n",
    "pipeline_distances = ColumnTransformer([('standardize', StandardScaler(), distances_to_standardize)],\n",
    "                            remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7040147-8c05-40c3-a1ba-5fa81a436c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_distances_stand = pd.DataFrame(pipeline_distances.fit_transform(events_df[distances_to_standardize]),\n",
    "                          index=events_df[distances_to_standardize].index.values ,\n",
    "                          columns=[x.split('__')[-1] for x in pipeline_distances.get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202dc9f-3c86-4c85-910f-22a0f00f45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_distance_data_stand = pd.DataFrame(pipeline_distances.transform(point_distance_data),\n",
    "                          index=point_distance_data.index,\n",
    "                          columns=[x.split('__')[-1] for x in pipeline_distances.get_feature_names_out()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535ee70-27e0-4a6f-be57-342662c9cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge point data\n",
    "final_point_df =  pd.concat([points_df_geo, point_distance_data_stand], axis =1 ).reset_index()\n",
    "\n",
    "final_point_df[\"is_island\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f54300-eb3a-40cc-a626-c2c9e45b7350",
   "metadata": {},
   "source": [
    "Now we add the climate and economic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d640a2a-aa61-44e4-bf2c-18af53c12fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAO_climate_predictions_req   = LAO_climate_predictions[[\"time\", \"co2\", \"Temp\", \"LAO_precip_deviation\",]].copy()\n",
    "LAO_economic_predictions_req = LAO_economic_predictions[[\"LAO_log_population\",\"LAO_log_gdp\",\n",
    "                                                         \"LAO_log_gdp_pc\" , \"ln_population_density\", \"Population\", \"time\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e2d4de-3c4b-49be-a848-e264eb117587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust ocean temp\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "stl_ocean_temp = STL(pd.DataFrame(LAO_climate_predictions_req[\"Temp\"].dropna()), period=3)\n",
    "result_ocean_temp = stl_ocean_temp.fit()\n",
    "trend_ocean_temp = result_ocean_temp.trend\n",
    "\n",
    "LAO_climate_predictions_req[\"Temp\"] = LAO_climate_predictions_req[\"Temp\"].dropna() - trend_ocean_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f05a18-bfdd-4756-9681-8419da033b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAO_economic_predictions_req.rename(columns = {\"ln_population_density\":\"log_population_density\",\n",
    "                                           \"LAO_log_gdp_pc\": \"log_gdp_per_cap\", }, inplace= True)\n",
    "\n",
    "LAO_climate_predictions_req.rename(columns = {\"LAO_precip_deviation\": \"precip_deviation\",\n",
    "                                                                    \"Temp\": \"dev_ocean_temp\",\n",
    "                                                                   }, inplace = True)\n",
    "\n",
    "Lao_prediction_inputs  = pd.merge(LAO_climate_predictions_req,LAO_economic_predictions_req, \n",
    "                                  left_on= \"time\", right_on=\"time\", how= \"left\"  )\n",
    "Lao_prediction_inputs = Lao_prediction_inputs[(time_varying_features_2 + [\"time\"]) ]\n",
    "Lao_prediction_inputs = Lao_prediction_inputs.dropna()\n",
    "Lao_prediction_inputs.rename(columns = {\"time\": \"Start_Year\"}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e4598-ff85-48d2-9b1b-5fb9335f0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize predictions\n",
    "\n",
    "#Train the pipeline on training data\n",
    "#Define pipeline\n",
    "\n",
    "cols_to_square = [\"log_population_density\", \"log_gdp_per_cap\"]\n",
    "\n",
    "pipeline_predictions = ColumnTransformer([('standardize', StandardScaler(), time_varying_features_2)],\n",
    "                                        remainder='passthrough')\n",
    "\n",
    "event_model_data_varying = pd.DataFrame(pipeline_predictions.fit_transform(events_df[time_varying_features_2 + [\"Start_Year\"]] ),\n",
    "                          index=events_df[time_varying_features_2].index,\n",
    "                          columns=[x.split('__')[-1] for x in pipeline_predictions.get_feature_names_out()])\n",
    "#Standardize\n",
    "Lao_prediction_inputs_stand = pd.DataFrame(pipeline_predictions.transform(Lao_prediction_inputs ),\n",
    "                          index=Lao_prediction_inputs[time_varying_features_2].index,\n",
    "                          columns=[x.split('__')[-1] for x in pipeline_predictions.get_feature_names_out()])\n",
    "\n",
    "Lao_prediction_inputs_stand[\"log_population_density_squared\"] =Lao_prediction_inputs_stand[\"log_population_density\"] **2\n",
    "\n",
    "Lao_prediction_inputs_stand[\"log_gdp_per_cap_squared\"] = Lao_prediction_inputs_stand[\"log_gdp_per_cap\"] ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d5b87-aa10-4a58-92af-2e9c65d76555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the data\n",
    "target_df = pd.DataFrame()\n",
    "\n",
    "for x in (years_to_predict):\n",
    "    temp = final_point_df.copy()\n",
    "    temp[\"Start_Year\"] = x\n",
    "    temp = pd.merge(temp, Lao_prediction_inputs_stand, left_on= \"Start_Year\", right_on= \"Start_Year\", \n",
    "                                  how = \"left\" )\n",
    "    target_df= pd.concat([target_df, temp], axis = 0 )\n",
    "\n",
    "target_df.reset_index(inplace= True, drop= True)\n",
    "target_df.drop(columns = [\"index\"], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaab80f-307d-4b10-9f90-a45b0233cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_predictions[\"time_varying_features\"] =  (time_varying_features_2 +\n",
    "                                                [\"log_gdp_per_cap_squared\", \"log_population_density_squared\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5b5c0-d497-48d5-8213-905a1a32bc1f",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866ff23-cf5a-4380-932e-9b8fba92321c",
   "metadata": {},
   "source": [
    "Now we run the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907498a7-8449-4418-8316-0f1bb1bfe814",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.set_index([\"city\", \"Start_Year\"]).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69374c-8089-4f3f-a090-23c4846e28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we run \n",
    "#Point prediction\n",
    "from copy import deepcopy\n",
    "from pymc.model.transform.optimization import freeze_dims_and_data\n",
    "coords_predictions['obs_idx'] = target_df.index.values\n",
    "coords_predictions[\"time_varying_features\"] =  (time_varying_features_2 +\n",
    "                                                [\"log_gdp_per_cap_squared\", \"log_population_density_squared\"])\n",
    "idx = xr.Coordinates.from_pandas_multiindex(target_df.set_index([\"city\", \"Start_Year\"]).index, \"obs_idx\" )\n",
    "\n",
    "\n",
    "with pm.Model(coords=coords_predictions) as damage_curves_plot:\n",
    "    ####################################Events model####################################\n",
    "    geographic_data = add_data(features= distance_features_2 ,  target = None, df =  target_df, dims=['obs_idx', 'features'])\n",
    "    economic_data = add_data(features = coords_predictions[\"time_varying_features\"] ,  target = None, df =  target_df,\n",
    "                             name='time_varying', dims=['obs_idx', 'time_varying_features'])\n",
    "    X_gp = pm.Data(\"X_gp\", target_df[[\"lat\", \"long\"]].astype(floatX), dims=['obs_idx', 'gp_feature'])\n",
    "\n",
    "    #Flat variables\n",
    "    country_effect = pm.Flat(\"country_effect\", dims = [\"ISO\"])    \n",
    "    geographic_betas = pm.Flat(\"geographic_betas\", dims = [\"distance_features\"])\n",
    "    economic_betas = pm.Flat(\"economic_betas\", dims = [\"time_varying_features\"])\n",
    "\n",
    "    # HSGP component\n",
    "    eta = pm.Flat(\"eta\")\n",
    "    ell = pm.Flat(\"ell\", dims=[\"gp_feature\"])\n",
    "    cov_func = eta**2 * pm.gp.cov.Matern52(input_dim=2, ls=ell)\n",
    "\n",
    "    m0, m1, c = 20, 20, 1.5\n",
    "    gp = pm.gp.HSGP(m=[m0, m1], c=c, cov_func=cov_func)\n",
    "    gp._X_center = compute_center(events_df[['lat', 'long']].values.astype(floatX))\n",
    "\n",
    "    phi, sqrt_psd = gp.prior_linearized(X=X_gp)\n",
    "\n",
    "    basis_coeffs = pm.Flat(\"basis_coeffs\", size=gp.n_basis_vectors)\n",
    "\n",
    "    HSGP_component = pm.Deterministic('HSGP_component', phi @ (basis_coeffs * sqrt_psd), dims=['obs_idx'])\n",
    "    geographic_component = pm.Deterministic('geographic_component', geographic_data @ geographic_betas, dims=['obs_idx'])\n",
    "    economic_component = pm.Deterministic('economic_component', economic_data @ economic_betas, dims=['obs_idx'])\n",
    "    \n",
    "    logit_p = pm.Deterministic('logit_p', country_effect[LAOS_idx] + geographic_component + economic_component + HSGP_component, dims=['obs_idx'])\n",
    "    event_prob = pm.Deterministic('event_prob', pm.math.invlogit(logit_p), dims=['obs_idx'])\n",
    "\n",
    "    ####################################Damages model####################################\n",
    "    damage_x_data = pm.Data(\"damage_x_data\",  target_df[coords_predictions[\"new_damage_features\"]], dims=['obs_idx', 'new_damage_features'])\n",
    "    country_effect_damage = pm.Flat(\"country_effect_damage\", dims = [\"ISO_damage\"])\n",
    "    beta = pm.Flat(\"beta\", dims = [\"new_damage_features\"])\n",
    "    sigma = pm.Flat(\"sigma\")\n",
    "\n",
    "    #mu and damage\n",
    "    mu =  country_effect_damage[63] + damage_x_data @ beta\n",
    "    predicted_log_damage = pm.Normal(\"predicted_log_damage\", mu = mu, sigma = sigma , dims = [\"obs_idx\"])\n",
    "    \n",
    "    predicted_damages = pm.Deterministic(\"predicted_damages\", pm.math.exp(predicted_log_damage), dims=['obs_idx'])\n",
    "\n",
    "    damages_curves = pm.Deterministic(\"damages_curves\", predicted_damages * event_prob, dims=['obs_idx'])\n",
    "    \n",
    "with freeze_dims_and_data(damage_curves_plot):\n",
    "    idata_plot_point = pm.sample_posterior_predictive(merged_posteriors , extend_inferencedata=False, \n",
    "                                                      compile_kwargs={'mode':'JAX'},\n",
    "                                                var_names=['HSGP_component', \n",
    "                                                           'geographic_component', \n",
    "                                                           'economic_component', \n",
    "                                                           'logit_p', \n",
    "                                                           'event_prob', \n",
    "                                                           \"predicted_log_damage\", \"predicted_damages\", \n",
    "                                                           \"damages_curves\"\n",
    "                                                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c23fe5-7766-4105-9355-5f7b3252e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_curves = idata_plot_point.posterior_predictive.mean(dim=(\"chain\", \"draw\")).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b00251-b4f2-42b9-8ea8-6a29b7a5a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_plot_point = idata_plot_point.assign_coords(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30665f-44cc-4c36-93f2-aaf0efa20757",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(idata_plot_point.posterior_predictive.sel(city = \"Vientiane\" ), var_names= [\"damages_curves\"], \n",
    "               combined= True, kind= \"ridgeplot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ac765-d7e1-4d83-be8b-d61fd4ddbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
