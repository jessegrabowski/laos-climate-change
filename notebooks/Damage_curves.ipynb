{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b65e0-e5d1-458a-83b2-27fe0b41ee9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d294f9-2547-4785-bd7b-fa83ecf7d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e47c0b2-1e0d-4015-a14a-63102a33ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "import numpyro\n",
    "numpyro.set_host_device_count(8)\n",
    "\n",
    "import sys\n",
    "from pyprojroot import here\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "from laos_gggi import load_all_data, load_gpcc_data, load_emdat_data, load_shapefile, load_wb_data\n",
    "from laos_gggi.data_functions.rivers_data_loader import load_rivers_data\n",
    "from laos_gggi.data_functions.rivers_damage import create_hydro_rivers_damage, create_floods_rivers_damage\n",
    "from laos_gggi.replication_data import create_replication_data\n",
    "from laos_gggi.const_vars import COUNTRIES_ISO, ISO_DICTIONARY, LAOS_LOCATION_DICTIONARY\n",
    "from laos_gggi.plotting import configure_plot_style, plot_ppc_loopit\n",
    "from laos_gggi.sample import sample_or_load\n",
    "from laos_gggi.statistics import get_distance_to_rivers\n",
    "\n",
    "\n",
    "configure_plot_style()\n",
    "SEED = sum(list(map(ord, 'climate_bayes')))\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446dbe1b-2dcd-46bd-a3de-2dd593c37ac7",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ae2cc6-1e91-48e4-82ef-dbf177539f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_country_effect():\n",
    "  with pm.modelcontext(None):\n",
    "    country_effect_mu = pm.Normal('country_effect_mu', mu = 0, sigma = 1)\n",
    "    country_effect_scale = pm.Gamma('country_effect_scale', alpha=2, beta=1)\n",
    "    country_effect_offset = pm.Normal('country_effect_offset', sigma=1, dims=\"ISO\")\n",
    "    country_effect = pm.Deterministic('country_effect', country_effect_mu + country_effect_scale * country_effect_offset, dims=\"ISO\")\n",
    "  return country_effect, country_effect_mu, country_effect_scale, country_effect_offset\n",
    "\n",
    "def add_data(features: list[str], target: str, df: pd.DataFrame, add_time :bool = False):\n",
    "    with pm.modelcontext(None):\n",
    "       X = pm.Data(\"X\", df[features], dims = [\"obs_idx\", \"feature\"] )\n",
    "       Y = pm.Data(\"Y\", df[target], dims = [\"obs_idx\"])\n",
    "       #time = pm.Data(\"time\", df[\"time_period\"])\n",
    "    if add_time:\n",
    "        return X, Y, time\n",
    "    else:\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15d49ad-5af1-4df6-b2dc-4508671024fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot inputs\n",
    "def generate_plot_inputs(target_variable: str, idata, disaster_type: str = \"hydrological_disasters\", df = pd.DataFrame ):\n",
    "    #Extract predictions\n",
    "    predictions = idata.posterior_predictive['y_hat'].mean(dim=['chain', 'draw'])\n",
    "    predictions = predictions.to_dataframe().drop(columns = [\"year\", \"ISO\"]).reset_index().rename(columns = {\"y_hat\": \"predictions\"})\n",
    "\n",
    "    hdi_mean = az.hdi(idata.posterior_predictive.y_hat)\n",
    "\n",
    "    hdi = hdi_mean['y_hat'].to_dataframe().drop(columns = [\"year\", \"ISO\"]).reset_index()\n",
    "\n",
    "    hdi_mean_50 = az.hdi(idata.posterior_predictive.y_hat, hdi_prob=.5)\n",
    "    \n",
    "    hdi_50 = hdi_mean_50['y_hat'].to_dataframe().drop(columns = [\"year\", \"ISO\"]).reset_index()\n",
    "\n",
    "    #Merge results and predictions in one df\n",
    "    df_predictions = df[[target_variable, \"ISO\", \"year\"]]\n",
    "\n",
    "    #95% HDI\n",
    "    df_predictions = ( pd.merge(df_predictions,  hdi.query('hdi == \"lower\"')[[\"ISO\", \"year\", \"y_hat\"]] , \n",
    "             left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"y_hat\": \"lower_y_hat_95\"}))\n",
    "    df_predictions = (pd.merge(df_predictions, hdi.query('hdi == \"higher\"')[[\"ISO\", \"year\", \"y_hat\"]] ,\n",
    "         left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"y_hat\": \"higher_y_hat_95\"}))\n",
    "    #50% HDI\n",
    "    df_predictions = ( pd.merge(df_predictions,  hdi_50.query('hdi == \"lower\"')[[\"ISO\", \"year\", \"y_hat\"]] , \n",
    "             left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"y_hat\": \"lower_y_hat_50\"}))\n",
    "    df_predictions = (pd.merge(df_predictions, hdi_50.query('hdi == \"higher\"')[[\"ISO\", \"year\", \"y_hat\"]] ,\n",
    "         left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "                     .rename(columns = {\"y_hat\": \"higher_y_hat_50\"}))\n",
    "    \n",
    "    #Predictions\n",
    "    df_predictions = (pd.merge(df_predictions, predictions ,\n",
    "             left_on= [\"ISO\", \"year\"], right_on= [\"ISO\", \"year\"], how = \"left\")\n",
    "             .rename(columns = {\"y_hat\": \"predictions\"}))\n",
    "    \n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6588022-e40b-4186-9de9-7639b3b48de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function\n",
    "def plotting_function(idata, country: str, df: pd.DataFrame, target_variable: str):\n",
    "    df_predictions = generate_plot_inputs(idata = idata, df = df, target_variable = target_variable)\n",
    "\n",
    "    #Filter country\n",
    "    data = df_predictions.query(\"ISO == @country\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(data[\"year\"], data[\"predictions\"], zorder=1000, color='tab:red', label=f'Mean Predicted {target_variable}')\n",
    "    ax.scatter(data[\"year\"], data[target_variable], color='k', label=f'Actual {target_variable}')\n",
    "    ax.fill_between(data[\"year\"], data[\"higher_y_hat_95\"], data[\"lower_y_hat_95\"], alpha=0.25, color='tab:blue', label='95% HDI')\n",
    "    ax.fill_between(data[\"year\"], data[\"lower_y_hat_50\"], data[\"higher_y_hat_50\"], alpha=0.5, color='tab:blue', label='50% HDI')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    #plt.title(f\"{country} disaster count and predictions\")\n",
    "\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(F\"{target_variable}\")\n",
    "    \n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e570a-2101-47f7-8808-ed1ba888007a",
   "metadata": {},
   "source": [
    "# Importing and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0442a900-3717-4c98-a3ed-2ddf1b023fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_df = create_replication_data()\n",
    "emdat = load_emdat_data()\n",
    "wb = load_wb_data()\n",
    "#Load shapefiles\n",
    "world = load_shapefile('world', repair_ISO_codes=True)\n",
    "laos = load_shapefile('laos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137e961e-6fbc-4bb4-833a-ea94cff7ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_df = create_hydro_rivers_damage()\n",
    "\n",
    "emdat_iso = damage_df.ISO.unique()\n",
    "world_iso = world.ISO_A3.unique()\n",
    "wb_iso = wb.index.get_level_values(0).unique()\n",
    "\n",
    "# Codes in shapefile but not in EMDAT\n",
    "common_codes = set(world_iso).intersection(set(emdat_iso)).intersection(set(wb_iso))\n",
    "damage_df = damage_df.query('ISO in @common_codes').copy()\n",
    "world = world.query('ISO_A3 in @common_codes').copy()\n",
    "wb = (wb.reset_index().query('country_code in @common_codes').assign(year = lambda x: pd.to_datetime(x.year, format='%Y')).copy())\n",
    "\n",
    "damage_df = (pd.merge(damage_df, wb, left_on=['ISO', 'year'], right_on=['country_code', 'year'], how='left')\n",
    "             .dropna(subset=['population_density', 'gdp_per_cap', 'Population']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50188a63-701d-423a-ac40-2cfa1d19d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_rivers = gpd.read_file(here('data/rivers/big_rivers.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ef8750-d1e2-40b4-95dd-7a9eebc37ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
